{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb6cc3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "416ee69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_path='../data/eaglei_data/'\n",
    "storm_path='../data/NOAA_StormEvents/'\n",
    "eaglei_2015=pd.read_csv(elec_path+'eaglei_outages_2015.csv')\n",
    "Storms=pd.read_csv(storm_path+'StormEvents_details-ftp_v1.0_d2015_c20240716.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d327613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_customers_out_by_date(df):\n",
    "    \"\"\"\n",
    "    Group the DataFrame by county, state, and the date part of run_start_time, \n",
    "    and sum the customers_out values.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Original data containing columns \n",
    "            ['fips_code', 'county', 'state', 'customers_out', 'run_start_time'].\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Grouped data with columns \n",
    "            ['county', 'state', 'date', 'customers_out', 'run_start_time'].\n",
    "    \"\"\"\n",
    "    # Make a copy of the DataFrame to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure 'run_start_time' is of datetime type\n",
    "    df['run_start_time'] = pd.to_datetime(df['run_start_time'])\n",
    "\n",
    "    # Extract the date part only\n",
    "    df['date'] = df['run_start_time'].dt.date\n",
    "\n",
    "    # Group by county, state, and date, and sum the customers_out\n",
    "    grouped = df.groupby(['fips_code', 'date']).agg({\n",
    "        'customers_out': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Add back a datetime version of the date if needed\n",
    "    grouped['run_start_time'] = pd.to_datetime(grouped['date'])\n",
    "\n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f610977",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outage=group_customers_out_by_date(eaglei_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08ef7bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jdsto\\AppData\\Local\\Temp\\ipykernel_15976\\277009826.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_storm['BEGIN_DATE_TIME'] = pd.to_datetime(df_storm['BEGIN_DATE_TIME']).dt.date\n",
      "C:\\Users\\jdsto\\AppData\\Local\\Temp\\ipykernel_15976\\277009826.py:3: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_storm['END_DATE_TIME'] = pd.to_datetime(df_storm['END_DATE_TIME']).dt.date\n"
     ]
    }
   ],
   "source": [
    "def change_date_storm(df_storm):\n",
    "    df_storm['BEGIN_DATE_TIME'] = pd.to_datetime(df_storm['BEGIN_DATE_TIME']).dt.date\n",
    "    df_storm['END_DATE_TIME'] = pd.to_datetime(df_storm['END_DATE_TIME']).dt.date\n",
    "    return df_storm\n",
    "df_storm=change_date_storm(Storms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31286b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jdsto\\AppData\\Local\\Temp\\ipykernel_15976\\1314662990.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  storms['STATE_FIPS'] = storms['STATE_FIPS'].astype(str).str.zfill(2)\n",
      "C:\\Users\\jdsto\\AppData\\Local\\Temp\\ipykernel_15976\\1314662990.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  storms['CZ_FIPS'] = storms['CZ_FIPS'].astype(str).str.zfill(3)\n",
      "C:\\Users\\jdsto\\AppData\\Local\\Temp\\ipykernel_15976\\1314662990.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  storms['FIPS'] = storms['STATE_FIPS'] + storms['CZ_FIPS']\n"
     ]
    }
   ],
   "source": [
    "def keep_c_storm(df_storm):\n",
    "    df_storm = df_storm[df_storm['CZ_TYPE'] == 'C']\n",
    "    return df_storm\n",
    "\n",
    "def make_fips_storm(storms):\n",
    "    storms['STATE_FIPS'] = storms['STATE_FIPS'].astype(str).str.zfill(2)\n",
    "    storms['CZ_FIPS'] = storms['CZ_FIPS'].astype(str).str.zfill(3)\n",
    "    storms['FIPS'] = storms['STATE_FIPS'] + storms['CZ_FIPS']\n",
    "    return storms\n",
    "\n",
    "df_storm=keep_c_storm(df_storm)\n",
    "df_storm=make_fips_storm(df_storm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "420cb779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating customers_out:   2%|‚ñè         | 11162/485528 [00:44<31:36, 250.11it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 55\u001b[0m\n\u001b[0;32m     48\u001b[0m         df_storm\u001b[38;5;241m.\u001b[39mloc[mask, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun_start_times\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_storm\u001b[38;5;241m.\u001b[39mloc[mask, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun_start_times\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m     49\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;241m+\u001b[39m [run_time]\n\u001b[0;32m     50\u001b[0m         )\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_storm\n\u001b[1;32m---> 55\u001b[0m df_combined\u001b[38;5;241m=\u001b[39mupdate_customers_out(df_storm, df_outage)\n",
      "Cell \u001b[1;32mIn[7], line 41\u001b[0m, in \u001b[0;36mupdate_customers_out\u001b[1;34m(df_storm, df_example)\u001b[0m\n\u001b[0;32m     37\u001b[0m fips_code \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfips_code\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Create a mask to find matching storm records\u001b[39;00m\n\u001b[0;32m     40\u001b[0m mask \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m---> 41\u001b[0m     (df_storm[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBEGIN_DATE_TIME\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m run_time) \u001b[38;5;241m&\u001b[39m \n\u001b[0;32m     42\u001b[0m     (df_storm[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEND_DATE_TIME\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m run_time) \u001b[38;5;241m&\u001b[39m \n\u001b[0;32m     43\u001b[0m     (df_storm[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFIPS\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m fips_code)\n\u001b[0;32m     44\u001b[0m )\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Update customers_out and append run_start_time to the list\u001b[39;00m\n\u001b[0;32m     47\u001b[0m df_storm\u001b[38;5;241m.\u001b[39mloc[mask, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomers_out\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m customers_out_value\n",
      "File \u001b[1;32mc:\\Users\\jdsto\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, other)\n",
      "File \u001b[1;32mc:\\Users\\jdsto\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:52\u001b[0m, in \u001b[0;36mOpsMixin.__le__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__le__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__le__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cmp_method(other, operator\u001b[38;5;241m.\u001b[39mle)\n",
      "File \u001b[1;32mc:\\Users\\jdsto\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6119\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6116\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   6117\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 6119\u001b[0m res_values \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mcomparison_op(lvalues, rvalues, op)\n\u001b[0;32m   6121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[1;32mc:\\Users\\jdsto\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:330\u001b[0m, in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    322\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLengths must match to compare\u001b[39m\u001b[38;5;124m\"\u001b[39m, lvalues\u001b[38;5;241m.\u001b[39mshape, rvalues\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    323\u001b[0m         )\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_extension_dispatch(lvalues, rvalues) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    326\u001b[0m     (\u001b[38;5;28misinstance\u001b[39m(rvalues, (Timedelta, BaseOffset, Timestamp)) \u001b[38;5;129;01mor\u001b[39;00m right \u001b[38;5;129;01mis\u001b[39;00m NaT)\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m lvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mobject\u001b[39m\n\u001b[0;32m    328\u001b[0m ):\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;66;03m# Call the method on lvalues\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m op(lvalues, rvalues)\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_scalar(rvalues) \u001b[38;5;129;01mand\u001b[39;00m isna(rvalues):  \u001b[38;5;66;03m# TODO: but not pd.NA?\u001b[39;00m\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;66;03m# numpy does not like comparisons vs None\u001b[39;00m\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m operator\u001b[38;5;241m.\u001b[39mne:\n",
      "File \u001b[1;32mc:\\Users\\jdsto\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, other)\n",
      "File \u001b[1;32mc:\\Users\\jdsto\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:52\u001b[0m, in \u001b[0;36mOpsMixin.__le__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__le__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__le__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cmp_method(other, operator\u001b[38;5;241m.\u001b[39mle)\n",
      "File \u001b[1;32mc:\\Users\\jdsto\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\datetimelike.py:1026\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   1024\u001b[0m o_mask \u001b[38;5;241m=\u001b[39m isna(other)\n\u001b[0;32m   1025\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_isnan \u001b[38;5;241m|\u001b[39m o_mask\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   1027\u001b[0m     nat_result \u001b[38;5;241m=\u001b[39m op \u001b[38;5;129;01mis\u001b[39;00m operator\u001b[38;5;241m.\u001b[39mne\n\u001b[0;32m   1028\u001b[0m     np\u001b[38;5;241m.\u001b[39mputmask(result, mask, nat_result)\n",
      "File \u001b[1;32mc:\\Users\\jdsto\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:55\u001b[0m, in \u001b[0;36m_any\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prod\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     52\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_prod(a, axis, dtype, out, keepdims, initial, where)\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_any\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Parsing keyword arguments is currently fairly slow, so avoid it for now\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m where \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m umr_any(a, axis, dtype, out, keepdims)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def update_customers_out(df_storm, df_example):\n",
    "    \"\"\"\n",
    "    Update the 'customers_out' column in df_storm based on matching time intervals \n",
    "    and FIPS codes from df_example, and store an array of run_start_time values \n",
    "    for matching outage events.\n",
    "\n",
    "    Args:\n",
    "        df_storm (pd.DataFrame): DataFrame containing storm events with columns \n",
    "            ['FIPS', 'BEGIN_DATE_TIME', 'END_DATE_TIME', ...].\n",
    "        df_example (pd.DataFrame): DataFrame containing customer outage events with columns \n",
    "            ['fips_code', 'customers_out', 'run_start_time', ...].\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated df_storm with 'customers_out' and 'run_start_times' columns modified.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure correct data types\n",
    "    df_storm = df_storm.copy()\n",
    "    df_storm['FIPS'] = df_storm['FIPS'].astype('int64')\n",
    "    df_storm['BEGIN_DATE_TIME'] = pd.to_datetime(df_storm['BEGIN_DATE_TIME'])\n",
    "    df_storm['END_DATE_TIME'] = pd.to_datetime(df_storm['END_DATE_TIME'])\n",
    "    \n",
    "    df_example = df_example.copy()\n",
    "    df_example['run_start_time'] = pd.to_datetime(df_example['run_start_time'])\n",
    "    df_example['fips_code'] = df_example['fips_code'].astype('int64')\n",
    "\n",
    "    # Initialize columns if they do not exist\n",
    "    if 'customers_out' not in df_storm.columns:\n",
    "        df_storm['customers_out'] = 0\n",
    "    if 'run_start_times' not in df_storm.columns:\n",
    "        df_storm['run_start_times'] = [[] for _ in range(len(df_storm))]\n",
    "\n",
    "    # Iterate over each row in df_example\n",
    "    for idx, row in tqdm(df_example.iterrows(), total=len(df_example), desc=\"Updating customers_out\"):\n",
    "        run_time = row['run_start_time']\n",
    "        customers_out_value = row['customers_out']\n",
    "        fips_code = row['fips_code']\n",
    "\n",
    "        # Create a mask to find matching storm records\n",
    "        mask = (\n",
    "            (df_storm['BEGIN_DATE_TIME'] <= run_time) & \n",
    "            (df_storm['END_DATE_TIME'] >= run_time) & \n",
    "            (df_storm['FIPS'] == fips_code)\n",
    "        )\n",
    "\n",
    "        # Update customers_out and append run_start_time to the list\n",
    "        df_storm.loc[mask, 'customers_out'] += customers_out_value\n",
    "        df_storm.loc[mask, 'run_start_times'] = df_storm.loc[mask, 'run_start_times'].apply(\n",
    "            lambda x: x + [run_time]\n",
    "        )\n",
    "\n",
    "    return df_storm\n",
    "\n",
    "\n",
    "df_combined=update_customers_out(df_storm, df_outage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d00e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lead_time_column(df_storm):\n",
    "    \"\"\"\n",
    "    Add a 'lead_time' column to df_storm, containing a list of time differences (in hours)\n",
    "    between each run_start_time and BEGIN_DATE_TIME. If run_start_times is empty, set lead_time to 0.\n",
    "\n",
    "    Args:\n",
    "        df_storm (pd.DataFrame): DataFrame with columns ['BEGIN_DATE_TIME', 'run_start_times', ...].\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated df_storm with a new 'lead_time' column.\n",
    "    \"\"\"\n",
    "    # Ensure df_storm is a copy to avoid modifying the input\n",
    "    df_storm = df_storm.copy()\n",
    "\n",
    "    # Initialize the lead_time column\n",
    "    df_storm['lead_time'] = df_storm.apply(\n",
    "        lambda row: [\n",
    "            (run_time - row['BEGIN_DATE_TIME']).total_seconds() / 86400  # Convert to hours\n",
    "            for run_time in row['run_start_times']\n",
    "        ] if row['run_start_times'] else [-1.0],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return df_storm\n",
    "\n",
    "df_combined = add_lead_time_column(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb41c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.to_csv('combined_data_2015.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
