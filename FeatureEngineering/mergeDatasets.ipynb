{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6cc3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d327613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_customers_out_by_date(df):\n",
    "    \"\"\"\n",
    "    Group the DataFrame by county, state, and the date part of run_start_time, \n",
    "    and sum the customers_out values.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Original data containing columns \n",
    "            ['fips_code', 'county', 'state', 'customers_out', 'run_start_time'].\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Grouped data with columns \n",
    "            ['county', 'state', 'date', 'customers_out', 'run_start_time'].\n",
    "    \"\"\"\n",
    "    # Make a copy of the DataFrame to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure 'run_start_time' is of datetime type\n",
    "    df['run_start_time'] = pd.to_datetime(df['run_start_time'])\n",
    "\n",
    "    # Extract the date part only\n",
    "    df['date'] = df['run_start_time'].dt.date\n",
    "\n",
    "    # Group by county, state, and date, and sum the customers_out\n",
    "    grouped = df.groupby(['fips_code', 'date']).agg({\n",
    "        'customers_out': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Add back a datetime version of the date if needed\n",
    "    grouped['run_start_time'] = pd.to_datetime(grouped['date'])\n",
    "\n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f610977",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_outage=group_customers_out_by_date(eaglei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ef7bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_date_storm(df_storm):\n",
    "    df_storm['BEGIN_DATE_TIME'] = pd.to_datetime(df_storm['BEGIN_DATE_TIME']).dt.date\n",
    "    df_storm['END_DATE_TIME'] = pd.to_datetime(df_storm['END_DATE_TIME']).dt.date\n",
    "    return df_storm\n",
    "#df_storm=change_date_storm(storms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31286b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_c_storm(df_storm):\n",
    "    df_storm = df_storm[df_storm['CZ_TYPE'] == 'C']\n",
    "    return df_storm\n",
    "\n",
    "def make_fips_storm(storms):\n",
    "    storms['STATE_FIPS'] = storms['STATE_FIPS'].astype(str).str.zfill(2)\n",
    "    storms['CZ_FIPS'] = storms['CZ_FIPS'].astype(str).str.zfill(3)\n",
    "    storms['FIPS'] = storms['STATE_FIPS'] + storms['CZ_FIPS']\n",
    "    return storms\n",
    "\n",
    "# df_storm=keep_c_storm(df_storm)\n",
    "# df_storm=make_fips_storm(df_storm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420cb779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_customers_out(df_storm, df_example):\n",
    "    \"\"\"\n",
    "    Update the 'customers_out' column in df_storm based on matching time intervals \n",
    "    and FIPS codes from df_example, and store an array of run_start_time values \n",
    "    for matching outage events.\n",
    "\n",
    "    Args:\n",
    "        df_storm (pd.DataFrame): DataFrame containing storm events with columns \n",
    "            ['FIPS', 'BEGIN_DATE_TIME', 'END_DATE_TIME', ...].\n",
    "        df_example (pd.DataFrame): DataFrame containing customer outage events with columns \n",
    "            ['fips_code', 'customers_out', 'run_start_time', ...].\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated df_storm with 'customers_out' and 'run_start_times' columns modified.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure correct data types\n",
    "    df_storm = df_storm.copy()\n",
    "    df_storm['FIPS'] = df_storm['FIPS'].astype('int64')\n",
    "    df_storm['BEGIN_DATE_TIME'] = pd.to_datetime(df_storm['BEGIN_DATE_TIME'])\n",
    "    df_storm['END_DATE_TIME'] = pd.to_datetime(df_storm['END_DATE_TIME'])\n",
    "    \n",
    "    df_example = df_example.copy()\n",
    "    df_example['run_start_time'] = pd.to_datetime(df_example['run_start_time'])\n",
    "    df_example['fips_code'] = df_example['fips_code'].astype('int64')\n",
    "\n",
    "    # Initialize columns if they do not exist\n",
    "    if 'customers_out' not in df_storm.columns:\n",
    "        df_storm['customers_out'] = 0\n",
    "    if 'run_start_times' not in df_storm.columns:\n",
    "        df_storm['run_start_times'] = [[] for _ in range(len(df_storm))]\n",
    "\n",
    "    # Iterate over each row in df_example\n",
    "    for idx, row in tqdm(df_example.iterrows(), total=len(df_example), desc=\"Updating customers_out\"):\n",
    "        run_time = row['run_start_time']\n",
    "        customers_out_value = row['customers_out']\n",
    "        fips_code = row['fips_code']\n",
    "\n",
    "        # Create a mask to find matching storm records\n",
    "        mask = (\n",
    "            (df_storm['BEGIN_DATE_TIME'] <= run_time) & \n",
    "            (df_storm['END_DATE_TIME'] >= run_time) & \n",
    "            (df_storm['FIPS'] == fips_code)\n",
    "        )\n",
    "\n",
    "        # Update customers_out and append run_start_time to the list\n",
    "        df_storm.loc[mask, 'customers_out'] += customers_out_value\n",
    "        df_storm.loc[mask, 'run_start_times'] = df_storm.loc[mask, 'run_start_times'].apply(\n",
    "            lambda x: x + [run_time]\n",
    "        )\n",
    "\n",
    "    return df_storm\n",
    "\n",
    "\n",
    "#df_combined=update_customers_out(df_storm, df_outage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d00e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lead_time_column(df_storm):\n",
    "    \"\"\"\n",
    "    Add a 'lead_time' column to df_storm, containing a list of time differences (in hours)\n",
    "    between each run_start_time and BEGIN_DATE_TIME. If run_start_times is empty, set lead_time to 0.\n",
    "\n",
    "    Args:\n",
    "        df_storm (pd.DataFrame): DataFrame with columns ['BEGIN_DATE_TIME', 'run_start_times', ...].\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated df_storm with a new 'lead_time' column.\n",
    "    \"\"\"\n",
    "    # Ensure df_storm is a copy to avoid modifying the input\n",
    "    df_storm = df_storm.copy()\n",
    "\n",
    "    # Initialize the lead_time column\n",
    "    df_storm['lead_time'] = df_storm.apply(\n",
    "        lambda row: [\n",
    "            (run_time - row['BEGIN_DATE_TIME']).total_seconds() / 86400  # Convert to hours\n",
    "            for run_time in row['run_start_times']\n",
    "        ] if row['run_start_times'] else [-1.0],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return df_storm\n",
    "\n",
    "#df_combined = add_lead_time_column(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8538931",
   "metadata": {},
   "outputs": [],
   "source": [
    "storm_names = ['StormEvents_details-ftp_v1.0_d2015_c20240716.csv', 'StormEvents_details-ftp_v1.0_d2016_c20220719.csv', \n",
    "               'StormEvents_details-ftp_v1.0_d2017_c20230317.csv', 'StormEvents_details-ftp_v1.0_d2018_c20240716.csv',\n",
    "               'StormEvents_details-ftp_v1.0_d2019_c20240117.csv', 'StormEvents_details-ftp_v1.0_d2020_c20240620.csv',\n",
    "               'StormEvents_details-ftp_v1.0_d2021_c20240716.csv', 'StormEvents_details-ftp_v1.0_d2022_c20241121.csv',\n",
    "               'StormEvents_details-ftp_v1.0_d2023_c20241216.csv']\n",
    "\n",
    "eaglei_names = ['eaglei_outages_2015.csv', 'eaglei_outages_2016.csv', 'eaglei_outages_2017.csv', 'eaglei_outages_2018.csv',\n",
    "                'eaglei_outages_2019.csv', 'eaglei_outages_2020.csv', 'eaglei_outages_2021.csv', 'eaglei_outages_2022.csv',\n",
    "                'eaglei_outages_2023.csv']\n",
    "\n",
    "data_path_eaglei = '../data/eaglei_data/'\n",
    "data_path_storms = '../data/NOAA_StormEvents/'\n",
    "\n",
    "all_years_combined = []\n",
    "\n",
    "for i in range(0, len(storm_names)):\n",
    "    eaglei_year=pd.read_csv(data_path_eaglei + eaglei_names[i])\n",
    "    storms_year=pd.read_csv(data_path_storms + storm_names[i])\n",
    "\n",
    "    df_outage=group_customers_out_by_date(eaglei_year)\n",
    "    df_storm=change_date_storm(storms_year)\n",
    "    df_storm=keep_c_storm(df_storm)\n",
    "    df_storm=make_fips_storm(df_storm)\n",
    "\n",
    "    df_combined=update_customers_out(df_storm, df_outage)\n",
    "    filename = f\"combined_{2014 + i}.csv\"\n",
    "    df_combined.to_csv(filename, index=False)\n",
    "    all_years_combined.append(df_combined)\n",
    "\n",
    "final_df = pd.concat(all_years_combined, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609de39a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb41c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('combined_data_all_years.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
