{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d779e7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3191952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/our_data/merged_2015.csv\",encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b6b7b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BEGIN_DATE'] = pd.to_datetime(df['BEGIN_DATE_TIME']).dt.date\n",
    "df['END_DATE'] = pd.to_datetime(df['END_DATE_TIME']).dt.date\n",
    "df['event_duration'] = (pd.to_datetime(df['END_DATE']) - pd.to_datetime(df['BEGIN_DATE'])).dt.days\n",
    "\n",
    "event_duration_map = df.groupby('EVENT_TYPE')['event_duration'].mean().to_dict()\n",
    "df['avg_event_duration_by_type'] = df['EVENT_TYPE'].map(event_duration_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f231dfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_map = {\n",
    "    'DEC': 'Winter', 'JAN': 'Winter', 'FEB': 'Winter',\n",
    "    'MAR': 'Spring', 'APR': 'Spring', 'MAY': 'Spring',\n",
    "    'JUN': 'Summer', 'JUL': 'Summer', 'AUG': 'Summer',\n",
    "    'SEP': 'Fall', 'OCT': 'Fall', 'NOV': 'Fall'\n",
    "}\n",
    "df['season'] = df['MONTH_NAME'].map(season_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dce5d904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jdsto\\AppData\\Local\\Temp\\ipykernel_16632\\3160111230.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['region_outage_freq'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "outage_freq = df[df['customers_out'] > 0].groupby('STATE').size()\n",
    "df['region_outage_freq'] = df['STATE'].map(outage_freq)\n",
    "df['region_outage_freq'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4dc7751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jdsto\\AppData\\Local\\Temp\\ipykernel_16632\\3886047351.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['TOR_F_SCALE_NUM'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "scale_map = {\n",
    "    'EF0': 0.5, 'EF1': 1, 'EF2': 2, 'EF3': 3,\n",
    "    'EF4': 4, 'EF5': 5, 'F0': 0.5, 'F1': 1, 'F2': 2,\n",
    "    'F3': 3, 'F4': 4, 'F5': 5\n",
    "}\n",
    "df['TOR_F_SCALE_NUM'] = df['TOR_F_SCALE'].map(scale_map)\n",
    "df['TOR_F_SCALE_NUM'].fillna(0, inplace=True)\n",
    "df['event_severity'] = df['MAGNITUDE'].fillna(0) + df['TOR_F_SCALE_NUM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e16b19c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_centers = df.groupby('STATE')[['BEGIN_LAT', 'BEGIN_LON']].mean()\n",
    "df = df.merge(state_centers, on='STATE', suffixes=('', '_CENTER'))\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # km\n",
    "    phi1, phi2 = np.radians(lat1), np.radians(lat2)\n",
    "    dphi = phi2 - phi1\n",
    "    dlambda = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dphi/2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda/2)**2\n",
    "    return 2 * R * np.arcsin(np.sqrt(a))\n",
    "\n",
    "df['dist_to_state_center'] = haversine(\n",
    "    df['BEGIN_LAT'], df['BEGIN_LON'],\n",
    "    df['BEGIN_LAT_CENTER'], df['BEGIN_LON_CENTER']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6cfce7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jdsto\\AppData\\Local\\Temp\\ipykernel_16632\\2698364759.py:60: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  source_validity = df.groupby('DATA_SOURCE').apply(lambda g: 1 - g.isnull().mean().mean()).to_dict()\n"
     ]
    }
   ],
   "source": [
    "monthly_outage_risk = df.groupby(['STATE', 'MONTH_NAME'])['customers_out'].mean().to_dict()\n",
    "df['monthly_outage_risk_index'] = df.apply(\n",
    "    lambda row: monthly_outage_risk.get((row['STATE'], row['MONTH_NAME']), 0), axis=1)\n",
    "\n",
    "event_freq = df['EVENT_TYPE'].value_counts().to_dict()\n",
    "df['event_type_freq'] = df['EVENT_TYPE'].map(event_freq)\n",
    "\n",
    "region_demand_proxy = df.groupby('STATE')['customers_out'].mean().to_dict()\n",
    "df['region_demand_proxy'] = df['STATE'].map(region_demand_proxy)\n",
    "\n",
    "df['BEGIN_HOUR'] = df['BEGIN_TIME'].astype(str).str.zfill(4).str[:2].astype(int)\n",
    "df['event_hour_norm'] = df['BEGIN_HOUR'] / 24.0\n",
    "\n",
    "min_year = df['YEAR'].min()\n",
    "df['year_trend'] = df['YEAR'] - min_year\n",
    "\n",
    "df['flood_cause_cat'] = df['FLOOD_CAUSE'].fillna('Unknown')\n",
    "df['flood_cause_cat'] = df['flood_cause_cat'].where(df['EVENT_TYPE'] == 'Flood', 'None')\n",
    "\n",
    "avg_length_map = df[df['TOR_LENGTH'].notna()].groupby('EVENT_TYPE')['TOR_LENGTH'].mean().to_dict()\n",
    "df['tornado_length_proxy'] = df.apply(\n",
    "    lambda row: avg_length_map.get(row['EVENT_TYPE'], 0) if 'Tornado' in row['EVENT_TYPE'] else 0, axis=1)\n",
    "\n",
    "avg_width_map = df[df['TOR_WIDTH'].notna()].groupby('EVENT_TYPE')['TOR_WIDTH'].mean().to_dict()\n",
    "df['tornado_width_proxy'] = df.apply(\n",
    "    lambda row: avg_width_map.get(row['EVENT_TYPE'], 0) if 'Tornado' in row['EVENT_TYPE'] else 0, axis=1)\n",
    "\n",
    "df['TOR_F_SCALE_NUM'] = df['TOR_F_SCALE'].map(scale_map).fillna(0)\n",
    "severity_by_region = df.groupby('STATE')[['MAGNITUDE', 'TOR_F_SCALE_NUM']].mean().sum(axis=1).to_dict()\n",
    "df['region_avg_severity'] = df['STATE'].map(severity_by_region)\n",
    "\n",
    "df['BEGIN_DATE'] = pd.to_datetime(df['BEGIN_DATE_TIME'])\n",
    "df['is_weekday'] = df['BEGIN_DATE'].dt.weekday < 5\n",
    "df['is_weekday'] = df['is_weekday'].astype(int)\n",
    "\n",
    "neighbor_impact = df.groupby('TOR_OTHER_CZ_STATE')['customers_out'].mean().to_dict()\n",
    "df['neighbor_outage_impact'] = df['TOR_OTHER_CZ_STATE'].map(neighbor_impact).fillna(0)\n",
    "\n",
    "wfo_freq = df['WFO'].value_counts().to_dict()\n",
    "df['wfo_influence'] = df['WFO'].map(wfo_freq).fillna(0)\n",
    "\n",
    "df['lat_grid'] = df['BEGIN_LAT'].round()\n",
    "df['lon_grid'] = df['BEGIN_LON'].round()\n",
    "grid_density = df.groupby(['lat_grid', 'lon_grid']).size().to_dict()\n",
    "df['grid_density'] = df.apply(lambda row: grid_density.get((row['lat_grid'], row['lon_grid']), 0), axis=1)\n",
    "\n",
    "df['BEGIN_DATE'] = pd.to_datetime(df['BEGIN_DATE_TIME'])\n",
    "\n",
    "df = df.sort_values('BEGIN_DATE')\n",
    "df['event_7day_density'] = 0\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    end_date = row['BEGIN_DATE']\n",
    "    start_date = end_date - timedelta(days=7)\n",
    "    same_region = (df['STATE'] == row['STATE']) & (df['BEGIN_DATE'] >= start_date) & (df['BEGIN_DATE'] < end_date)\n",
    "    df.at[idx, 'event_7day_density'] = same_region.sum()\n",
    "\n",
    "df['event_season_combo'] = df['EVENT_TYPE'].astype(str) + '_' + df['season'].astype(str)\n",
    "\n",
    "source_validity = df.groupby('DATA_SOURCE').apply(lambda g: 1 - g.isnull().mean().mean()).to_dict()\n",
    "df['data_source_reliability'] = df['DATA_SOURCE'].map(source_validity).fillna(0)\n",
    "\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['BEGIN_DATE'].dt.month / 12)\n",
    "\n",
    "load_proxy = df.groupby('STATE')['customers_out'].mean().to_dict()\n",
    "df['grid_load_proxy'] = df['STATE'].map(load_proxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649079e0",
   "metadata": {},
   "source": [
    "JORDAN PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83b23eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"chatgpt_features_full.csv\",encoding='latin1')\n",
    "# print(df.columns.tolist())\n",
    "\n",
    "df['FIPS'] = df['STATE_FIPS'].astype(str).str.zfill(2) + df['CZ_FIPS'].astype(str).str.zfill(3)\n",
    "df['FIPS'] = df['FIPS'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a82d61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[['customers_out', 'BEGIN_DATE_TIME', 'END_DATE_TIME',\n",
    "    'BEGIN_LAT', 'BEGIN_LON', 'grid_density', 'grid_load_proxy', 'month_sin', 'is_weekday', 'region_avg_severity',\n",
    "    'event_severity', 'EVENT_TYPE', 'avg_event_duration_by_type', 'region_outage_freq', 'FIPS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec1f71f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['customers_out', 'BEGIN_LAT', 'BEGIN_LON', 'grid_density', 'grid_load_proxy', 'month_sin', 'is_weekday', 'region_avg_severity', 'event_severity', 'avg_event_duration_by_type', 'region_outage_freq', 'FIPS', 'begin_month', 'begin_weekday', 'event_type_num']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jdsto\\AppData\\Local\\Temp\\ipykernel_16632\\3175783415.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  features['BEGIN_DATE_TIME'] = pd.to_datetime(features['BEGIN_DATE_TIME'])\n",
      "C:\\Users\\jdsto\\AppData\\Local\\Temp\\ipykernel_16632\\3175783415.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  features['END_DATE_TIME'] = pd.to_datetime(features['END_DATE_TIME'])\n",
      "C:\\Users\\jdsto\\AppData\\Local\\Temp\\ipykernel_16632\\3175783415.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  features['begin_month'] = features['BEGIN_DATE_TIME'].dt.month\n",
      "C:\\Users\\jdsto\\AppData\\Local\\Temp\\ipykernel_16632\\3175783415.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  features['begin_weekday'] = features['BEGIN_DATE_TIME'].dt.dayofweek  # Monday=0\n",
      "C:\\Users\\jdsto\\AppData\\Local\\Temp\\ipykernel_16632\\3175783415.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  features['event_type_num'] = features['EVENT_TYPE'].apply(map_event_to_num)\n"
     ]
    }
   ],
   "source": [
    "#change begin and end to datetime\n",
    "features['BEGIN_DATE_TIME'] = pd.to_datetime(features['BEGIN_DATE_TIME'])\n",
    "features['END_DATE_TIME'] = pd.to_datetime(features['END_DATE_TIME'])\n",
    "\n",
    "#extract features\n",
    "features['begin_month'] = features['BEGIN_DATE_TIME'].dt.month\n",
    "features['begin_weekday'] = features['BEGIN_DATE_TIME'].dt.dayofweek  # Monday=0\n",
    "\n",
    "#damage features are given in weird string format \n",
    "#features['DAMAGE_PROPERTY'] = features['DAMAGE_PROPERTY'].apply(parse_k).astype(int)\n",
    "#features['DAMAGE_CROPS'] = features['DAMAGE_CROPS'].apply(parse_k).astype(int)\n",
    "\n",
    "event_mapping = {event: idx for idx, event in enumerate(sorted(features['EVENT_TYPE'].unique()))}\n",
    "\n",
    "def map_event_to_num(event):\n",
    "    return event_mapping.get(event, -1)\n",
    "\n",
    "features['event_type_num'] = features['EVENT_TYPE'].apply(map_event_to_num)\n",
    "\n",
    "features = features.drop(columns=['BEGIN_DATE_TIME', 'END_DATE_TIME', 'EVENT_TYPE'])\n",
    "print(features.columns.tolist())\n",
    "features = features.dropna() #only one row contains NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaa7954",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.to_csv(\"featuresXGBoost.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
