{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=pd.read_csv('data_leadtime.csv')\n",
    "data=pd.read_csv('all_years_combined.csv')\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Property and Crops into numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_damage(damage_str):\n",
    "    if pd.isna(damage_str):\n",
    "        return 0\n",
    "    damage_str = damage_str.upper().strip()\n",
    "    if damage_str[-1] == 'K':\n",
    "        return float(damage_str[:-1]) * 1_000\n",
    "    elif damage_str[-1] == 'M':\n",
    "        return float(damage_str[:-1]) * 1_000_000\n",
    "    elif damage_str[-1] == 'B':\n",
    "        return float(damage_str[:-1]) * 1_000_000_000\n",
    "    else:\n",
    "        # 如果没有单位，尝试直接转换\n",
    "        try:\n",
    "            return float(damage_str)\n",
    "        except ValueError:\n",
    "            return 0  # 非法字符串默认返回 0\n",
    "\n",
    "# 应用函数转换\n",
    "train['DAMAGE_PROPERTY'] = train['DAMAGE_PROPERTY'].apply(parse_damage).astype(int)\n",
    "train['DAMAGE_CROPS'] = train['DAMAGE_CROPS'].apply(parse_damage).astype(int) \n",
    "\n",
    "test['DAMAGE_PROPERTY'] = test['DAMAGE_PROPERTY'].apply(parse_damage).astype(int)\n",
    "test['DAMAGE_CROPS'] = test['DAMAGE_CROPS'].apply(parse_damage).astype(int) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/02/n1hnl_j93_d4b92q_jxc8rz80000gn/T/ipykernel_22911/1884665913.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  train['BEGIN_DATE'] = pd.to_datetime(train['BEGIN_DATE_TIME']).dt.date\n",
      "/var/folders/02/n1hnl_j93_d4b92q_jxc8rz80000gn/T/ipykernel_22911/1884665913.py:3: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  train['END_DATE'] = pd.to_datetime(train['END_DATE_TIME']).dt.date\n",
      "/var/folders/02/n1hnl_j93_d4b92q_jxc8rz80000gn/T/ipykernel_22911/1884665913.py:54: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  train['BEGIN_DATE'] = pd.to_datetime(train['BEGIN_DATE_TIME'])\n",
      "/var/folders/02/n1hnl_j93_d4b92q_jxc8rz80000gn/T/ipykernel_22911/1884665913.py:83: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  source_validity = train.groupby('DATA_SOURCE').apply(lambda g: 1 - g.isnull().mean().mean()).to_dict()\n"
     ]
    }
   ],
   "source": [
    "# Convert basic fields first\n",
    "train['BEGIN_DATE'] = pd.to_datetime(train['BEGIN_DATE_TIME']).dt.date\n",
    "train['END_DATE'] = pd.to_datetime(train['END_DATE_TIME']).dt.date\n",
    "# train['BEGIN_HOUR'] = train['BEGIN_TIME'].astype(str).str.zfill(4).str[:2].astype(int)\n",
    "\n",
    "train['event_duration'] = (pd.to_datetime(train['END_DATE']) - pd.to_datetime(train['BEGIN_DATE'])).dt.days\n",
    "\n",
    "# Average event duration by event type\n",
    "event_duration_map = train.groupby('EVENT_TYPE')['event_duration'].mean().to_dict()\n",
    "train['avg_event_duration_by_type'] = train['EVENT_TYPE'].map(event_duration_map)\n",
    "\n",
    "# Historical outage frequency by region\n",
    "outage_counts = train[train['customers_out_sum'] > 0].groupby('STATE').size().to_dict()\n",
    "train['region_outage_freq'] = train['STATE'].map(outage_counts).fillna(0)\n",
    "\n",
    "# Distance to state center (approximated using mean latitude and longitude)\n",
    "state_centers = train.groupby('STATE')[['BEGIN_LAT', 'BEGIN_LON']].mean().to_dict('index')\n",
    "train['dist_to_state_center'] = train.apply(\n",
    "    lambda row: np.sqrt(\n",
    "        (row['BEGIN_LAT'] - state_centers.get(row['STATE'], {'BEGIN_LAT':0})['BEGIN_LAT'])**2 +\n",
    "        (row['BEGIN_LON'] - state_centers.get(row['STATE'], {'BEGIN_LON':0})['BEGIN_LON'])**2\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# Monthly outage risk index\n",
    "monthly_outage_risk = train.groupby(['STATE', 'MONTH_NAME'])['customers_out_sum'].mean().to_dict()\n",
    "train['monthly_outage_risk_index'] = train.apply(\n",
    "    lambda row: monthly_outage_risk.get((row['STATE'], row['MONTH_NAME']), 0), axis=1)\n",
    "\n",
    "# Frequency of each event type\n",
    "event_freq = train['EVENT_TYPE'].value_counts().to_dict()\n",
    "train['event_type_freq'] = train['EVENT_TYPE'].map(event_freq)\n",
    "\n",
    "# Proxy for regional population demand\n",
    "region_demand_proxy = train.groupby('STATE')['customers_out_sum'].mean().to_dict()\n",
    "train['region_demand_proxy'] = train['STATE'].map(region_demand_proxy)\n",
    "\n",
    "# Yearly trend factor\n",
    "min_year = train['YEAR'].min()\n",
    "train['year_trend'] = train['YEAR'] - min_year\n",
    "\n",
    "# Whether it's a weekday\n",
    "train['BEGIN_DATE'] = pd.to_datetime(train['BEGIN_DATE_TIME'])\n",
    "train['is_weekday'] = train['BEGIN_DATE'].dt.weekday < 5\n",
    "train['is_weekday'] = train['is_weekday'].astype(int)\n",
    "\n",
    "# Neighboring region impact factor\n",
    "neighbor_impact = train.groupby('TOR_OTHER_CZ_STATE')['customers_out_sum'].mean().to_dict()\n",
    "train['neighbor_outage_impact'] = train['TOR_OTHER_CZ_STATE'].map(neighbor_impact).fillna(0)\n",
    "\n",
    "# Weather forecast office influence\n",
    "wfo_freq = train['WFO'].value_counts().to_dict()\n",
    "train['wfo_influence'] = train['WFO'].map(wfo_freq).fillna(0)\n",
    "\n",
    "# Latitude and longitude grid density\n",
    "train['lat_grid'] = train['BEGIN_LAT'].round()\n",
    "train['lon_grid'] = train['BEGIN_LON'].round()\n",
    "grid_density = train.groupby(['lat_grid', 'lon_grid']).size().to_dict()\n",
    "train['grid_density'] = train.apply(lambda row: grid_density.get((row['lat_grid'], row['lon_grid']), 0), axis=1)\n",
    "\n",
    "# Event density in the past 7 days (may be inefficient for large datasets, consider optimization)\n",
    "train = train.sort_values('BEGIN_DATE')\n",
    "train['event_7day_density'] = 0\n",
    "for idx, row in train.iterrows():\n",
    "    end_date = row['BEGIN_DATE']\n",
    "    start_date = end_date - timedelta(days=7)\n",
    "    same_region = (train['STATE'] == row['STATE']) & (train['BEGIN_DATE'] >= start_date) & (train['BEGIN_DATE'] < end_date)\n",
    "    train.at[idx, 'event_7day_density'] = same_region.sum()\n",
    "\n",
    "# Reliability of historical data sources\n",
    "source_validity = train.groupby('DATA_SOURCE').apply(lambda g: 1 - g.isnull().mean().mean()).to_dict()\n",
    "train['data_source_reliability'] = train['DATA_SOURCE'].map(source_validity).fillna(0)\n",
    "\n",
    "# Month as a cyclic feature\n",
    "train['month_sin'] = np.sin(2 * np.pi * train['BEGIN_DATE'].dt.month / 12)\n",
    "\n",
    "# Proxy for regional power grid load\n",
    "load_proxy = train.groupby('STATE')['customers_out_sum'].mean().to_dict()\n",
    "train['grid_load_proxy'] = train['STATE'].map(load_proxy)\n",
    "\n",
    "# Calculate average damage per FIPS region\n",
    "train['DAMAGE_PROPERTY_MEAN'] = train.groupby('FIPS')['DAMAGE_PROPERTY'].transform('mean')\n",
    "train['DAMAGE_CROPS_MEAN'] = train.groupby('FIPS')['DAMAGE_CROPS'].transform('mean')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Train Set Features data into Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/02/n1hnl_j93_d4b92q_jxc8rz80000gn/T/ipykernel_22911/3036850015.py:5: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  test['BEGIN_DATE'] = pd.to_datetime(test['BEGIN_DATE_TIME']).dt.date\n",
      "/var/folders/02/n1hnl_j93_d4b92q_jxc8rz80000gn/T/ipykernel_22911/3036850015.py:6: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  test['END_DATE'] = pd.to_datetime(test['END_DATE_TIME']).dt.date\n",
      "/var/folders/02/n1hnl_j93_d4b92q_jxc8rz80000gn/T/ipykernel_22911/3036850015.py:37: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  test['BEGIN_DATE'] = pd.to_datetime(test['BEGIN_DATE_TIME'])\n"
     ]
    }
   ],
   "source": [
    "# Assume test is already loaded\n",
    "test = test.copy()\n",
    "\n",
    "# Date conversion\n",
    "test['BEGIN_DATE'] = pd.to_datetime(test['BEGIN_DATE_TIME']).dt.date\n",
    "test['END_DATE'] = pd.to_datetime(test['END_DATE_TIME']).dt.date\n",
    "test['event_duration'] = (pd.to_datetime(test['END_DATE']) - pd.to_datetime(test['BEGIN_DATE'])).dt.days\n",
    "\n",
    "# Average event duration by event type\n",
    "test['avg_event_duration_by_type'] = test['EVENT_TYPE'].map(event_duration_map)\n",
    "\n",
    "# Historical outage frequency by region\n",
    "test['region_outage_freq'] = test['STATE'].map(outage_counts).fillna(0)\n",
    "\n",
    "# Distance to state center\n",
    "test['dist_to_state_center'] = test.apply(\n",
    "    lambda row: np.sqrt(\n",
    "        (row['BEGIN_LAT'] - state_centers.get(row['STATE'], {'BEGIN_LAT': 0})['BEGIN_LAT'])**2 +\n",
    "        (row['BEGIN_LON'] - state_centers.get(row['STATE'], {'BEGIN_LON': 0})['BEGIN_LON'])**2\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# Monthly outage risk index\n",
    "test['monthly_outage_risk_index'] = test.apply(\n",
    "    lambda row: monthly_outage_risk.get((row['STATE'], row['MONTH_NAME']), 0), axis=1)\n",
    "\n",
    "# Event type frequency\n",
    "test['event_type_freq'] = test['EVENT_TYPE'].map(event_freq)\n",
    "\n",
    "# Proxy for regional population demand\n",
    "test['region_demand_proxy'] = test['STATE'].map(region_demand_proxy)\n",
    "\n",
    "# Yearly trend factor\n",
    "test['year_trend'] = test['YEAR'] - min_year\n",
    "\n",
    "# Whether it is a weekday\n",
    "test['BEGIN_DATE'] = pd.to_datetime(test['BEGIN_DATE_TIME'])\n",
    "test['is_weekday'] = (test['BEGIN_DATE'].dt.weekday < 5).astype(int)\n",
    "\n",
    "# Neighboring region impact factor\n",
    "test['neighbor_outage_impact'] = test['TOR_OTHER_CZ_STATE'].map(neighbor_impact).fillna(0)\n",
    "\n",
    "# Weather forecast office influence\n",
    "test['wfo_influence'] = test['WFO'].map(wfo_freq).fillna(0)\n",
    "\n",
    "# Latitude and longitude grid density\n",
    "test['lat_grid'] = test['BEGIN_LAT'].round()\n",
    "test['lon_grid'] = test['BEGIN_LON'].round()\n",
    "test['grid_density'] = test.apply(\n",
    "    lambda row: grid_density.get((row['lat_grid'], row['lon_grid']), 0), axis=1\n",
    ")\n",
    "\n",
    "# Reliability of historical data sources\n",
    "test['data_source_reliability'] = test['DATA_SOURCE'].map(source_validity).fillna(0)\n",
    "\n",
    "# Month as a cyclic feature\n",
    "test['month_sin'] = np.sin(2 * np.pi * test['BEGIN_DATE'].dt.month / 12)\n",
    "\n",
    "# Proxy for regional power grid load\n",
    "test['grid_load_proxy'] = test['STATE'].map(load_proxy)\n",
    "\n",
    "# FIPS-related averages (use merge for safety)\n",
    "fips_property_mean = train.groupby('FIPS')['DAMAGE_PROPERTY'].mean().reset_index().rename(columns={'DAMAGE_PROPERTY': 'DAMAGE_PROPERTY_MEAN'})\n",
    "fips_crops_mean = train.groupby('FIPS')['DAMAGE_CROPS'].mean().reset_index().rename(columns={'DAMAGE_CROPS': 'DAMAGE_CROPS_MEAN'})\n",
    "test = test.merge(fips_property_mean, on='FIPS', how='left')\n",
    "test = test.merge(fips_crops_mean, on='FIPS', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep Columns The Same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_columns = test.columns\n",
    "\n",
    "train = train[test_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check NAN values and INF values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary module\n",
    "import numpy as np\n",
    "\n",
    "# Check for missing and infinite values in the training set\n",
    "print(\"=== Train NaN Summary ===\")\n",
    "print(train.isna().sum()[train.isna().sum() > 0].sort_values(ascending=False))\n",
    "\n",
    "print(\"\\n=== Train Inf Summary ===\")\n",
    "print((~np.isfinite(train.select_dtypes(include=[np.number]))).sum().sort_values(ascending=False))\n",
    "\n",
    "# Check for missing and infinite values in the test set\n",
    "print(\"\\n=== Test NaN Summary ===\")\n",
    "print(test.isna().sum()[test.isna().sum() > 0].sort_values(ascending=False))\n",
    "\n",
    "print(\"\\n=== Test Inf Summary ===\")\n",
    "print((~np.isfinite(test.select_dtypes(include=[np.number]))).sum().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('final_leadtimeData_train.csv', index=False)\n",
    "test.to_csv('final_leadtimeData_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
