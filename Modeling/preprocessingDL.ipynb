{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_path='data/eaglei_data/'\n",
    "storm_path='data/NOAA_StormEvents/'\n",
    "eaglei_2015=pd.read_csv(elec_path+'eaglei_outages_2015.csv')\n",
    "Storms_2015=pd.read_csv(storm_path+'StormEvents_details-ftp_v1.0_d2015_c20240716.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate 'STATE_FIPS' and 'CZ_FIPS' into \"FIPS\" in Storms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_c_storm(Storms_2015):\n",
    "    Storms_2015 = Storms_2015[Storms_2015['CZ_TYPE'] == 'C']\n",
    "    return Storms_2015\n",
    "\n",
    "def make_fips_storm(storms):\n",
    "    storms['STATE_FIPS'] = storms['STATE_FIPS'].astype(str).str.zfill(2)\n",
    "    storms['CZ_FIPS'] = storms['CZ_FIPS'].astype(str).str.zfill(3)\n",
    "    storms['FIPS'] = storms['STATE_FIPS'] + storms['CZ_FIPS']\n",
    "    return storms\n",
    "\n",
    "# Storms_2015=keep_c_storm(Storms_2015)\n",
    "# Storms_2015=make_fips_storm(Storms_2015)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate 'BEGIN_YEARMONTH', 'BEGIN_DAY', 'BEGIN_TIME' into \"BEGIN\" and \"END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_datetime(df):\n",
    "    # Function to convert the 'BEGIN' and 'END' columns to datetime\n",
    "    def convert_datetime(yearmonth, day, time):\n",
    "        return pd.to_datetime(\n",
    "            yearmonth.astype(str).str[:4] + '-' +  # Year\n",
    "            yearmonth.astype(str).str[4:6] + '-' +  # Month\n",
    "            day.astype(str).str.zfill(2) + ' ' +  # Day\n",
    "            time.astype(str).str.zfill(4).str[:2] + ':' +  # Hour\n",
    "            time.astype(str).str.zfill(4).str[2:]  # Minute\n",
    "        )\n",
    "    \n",
    "    # Apply conversion to both BEGIN and END columns\n",
    "    df['BEGIN'] = convert_datetime(df['BEGIN_YEARMONTH'], df['BEGIN_DAY'], df['BEGIN_TIME'])\n",
    "    df['END'] = convert_datetime(df['END_YEARMONTH'], df['END_DAY'], df['END_TIME'])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matches storm events with outage records by time and location\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_intervals_and_aggregate_full(df, eaglei_2015):\n",
    "    df['BEGIN'] = pd.to_datetime(df['BEGIN'])\n",
    "    df['END'] = pd.to_datetime(df['END'])\n",
    "    eaglei_2015['run_start_time'] = pd.to_datetime(eaglei_2015['run_start_time'])\n",
    "\n",
    "    df['FIPS'] = pd.to_numeric(df['FIPS'], errors='coerce').astype('Int64')\n",
    "    eaglei_2015['fips_code'] = pd.to_numeric(eaglei_2015['fips_code'], errors='coerce').astype('Int64')\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # 构建 eaglei 分组\n",
    "    run_grouped = eaglei_2015.groupby('fips_code')\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        fips = row['FIPS']\n",
    "        begin, end = row['BEGIN'], row['END']\n",
    "\n",
    "        # 默认空匹配\n",
    "        customers_out_sum = 0\n",
    "        run_start_time_mean = pd.NaT\n",
    "        interval_count = 0\n",
    "\n",
    "        if fips in run_grouped.groups:\n",
    "            run_part = run_grouped.get_group(fips)\n",
    "            matched = run_part[(run_part['run_start_time'] >= begin) & (run_part['run_start_time'] <= end)]\n",
    "\n",
    "            if not matched.empty:\n",
    "                customers_out_sum = matched['customers_out'].sum()\n",
    "                run_start_time_mean = matched['run_start_time'].mean()\n",
    "                interval_count = matched.shape[0]\n",
    "\n",
    "        combined_row = row.to_dict()\n",
    "        combined_row.update({\n",
    "            'customers_out_sum': customers_out_sum,\n",
    "            'run_start_time_mean': run_start_time_mean,\n",
    "            'interval_count': interval_count\n",
    "        })\n",
    "        results.append(combined_row)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "# results = match_intervals_and_aggregate_full(Storms_2015, eaglei_2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Storm data and eaglei data year by year and concatenate all years "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storm_names = ['StormEvents_details-ftp_v1.0_d2015_c20240716.csv', 'StormEvents_details-ftp_v1.0_d2016_c20220719.csv', \n",
    "               'StormEvents_details-ftp_v1.0_d2017_c20230317.csv', 'StormEvents_details-ftp_v1.0_d2018_c20240716.csv',\n",
    "               'StormEvents_details-ftp_v1.0_d2019_c20240117.csv', 'StormEvents_details-ftp_v1.0_d2020_c20240620.csv',\n",
    "               'StormEvents_details-ftp_v1.0_d2021_c20240716.csv', 'StormEvents_details-ftp_v1.0_d2022_c20241121.csv',\n",
    "               'StormEvents_details-ftp_v1.0_d2023_c20241216.csv']\n",
    "\n",
    "eaglei_names = ['eaglei_outages_2015.csv', 'eaglei_outages_2016.csv', 'eaglei_outages_2017.csv', 'eaglei_outages_2018.csv',\n",
    "                'eaglei_outages_2019.csv', 'eaglei_outages_2020.csv', 'eaglei_outages_2021.csv', 'eaglei_outages_2022.csv',\n",
    "                'eaglei_outages_2023.csv']\n",
    "\n",
    "data_path_eaglei = 'data/eaglei_data/'\n",
    "data_path_storms = 'data/NOAA_StormEvents/'\n",
    "\n",
    "all_years_combined = []\n",
    "\n",
    "for i in range(0, len(storm_names)):\n",
    "    eaglei_year=pd.read_csv(data_path_eaglei + eaglei_names[i])\n",
    "    storms_year=pd.read_csv(data_path_storms + storm_names[i])\n",
    "\n",
    "    storms_year=keep_c_storm(storms_year)\n",
    "    storms_year=make_fips_storm(storms_year)\n",
    "\n",
    "    storms_year=convert_to_datetime(storms_year)\n",
    "\n",
    "    df_combined=match_intervals_and_aggregate_full(storms_year, eaglei_year)\n",
    "    filename = f\"combined_{2014 + i}.csv\"\n",
    "    df_combined.to_csv(filename, index=False)\n",
    "    all_years_combined.append(df_combined)\n",
    "    # print(i)\n",
    "final_df = pd.concat(all_years_combined, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save \"all_year_combined.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('all_years_combined.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
